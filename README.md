# CryptoStream â€” Real-Time Analytics Pipeline

> **M2 BDIA â€” Data Streaming Project Â· 2025/2026**  
> UniversitÃ© Paris Dauphine

---

## ðŸ‘¥ Team

**Molka ESSID**
**Mehdi ZNAIDI** 
**Nour SAHLI** 

---

## ðŸ“Œ Project Overview

CryptoStream is a fully real-time data streaming application that:

1. **Ingests** live cryptocurrency trade data from the Binance WebSocket API
2. **Processes** the stream through a multi-step data treatment pipeline
3. **Aggregates** trades using Spark Structured Streaming with tumbling and sliding windows
4. **Applies Machine Learning** â€” anomaly detection and price prediction on the live stream
5. **Visualizes** everything in a live auto-refreshing Streamlit dashboard

All data is real â€” every message is an actual trade happening on Binance at that exact moment.

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BINANCE WEBSOCKET API                        â”‚
â”‚                   (BTC/USDT & ETH/USDT live trades)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    producer.py                       â”‚
â”‚                                                                     â”‚
â”‚  Step 1+2 â”‚ Field validation & type conversion                      â”‚
â”‚  Step 3   â”‚ Deduplication (rolling cache of 2000 trade IDs)        â”‚
â”‚  Step 4   â”‚ Price spike detection (flag if jump > 2%)              â”‚
â”‚  Step 5   â”‚ Enrichment (trade_value_usdt, ISO timestamp, trade_id) â”‚
â”‚  Step 6   â”‚ Live stats tracking (avg/min/max price, volume)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”˜
       â”‚                            â”‚
       â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ raw_trades  â”‚            â”‚  clean_trades   â”‚  â† Kafka Topics
â”‚  (Kafka)    â”‚            â”‚   (Kafka)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   spark/app.py                       â”‚
â”‚                                                                     â”‚
â”‚  â€¢ Schema parsing & type casting                                    â”‚
â”‚  â€¢ Watermark (2 min) + deduplication by trade_id                   â”‚
â”‚  â€¢ Tumbling window (1 min): trade_count, avg_price,                â”‚
â”‚    sum_qty, sum_value, spike_count                                  â”‚
â”‚  â€¢ Sliding window (5 min / 1 min slide): trade_count,              â”‚
â”‚    avg_price, min_price, max_price, sum_value                      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”˜
       â”‚                            â”‚
       â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ tumbling_1m/     â”‚     â”‚ sliding_5m_1m/        â”‚  â† Parquet Files
â”‚ (parquet)        â”‚     â”‚ (parquet)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               dashboard.py + ml_model.py           â”‚
â”‚                                                                     â”‚
â”‚  â€¢ Isolation Forest â€” anomaly detection on streaming windows       â”‚
â”‚  â€¢ Linear Regression â€” next window price prediction                â”‚
â”‚  â€¢ Streamlit dashboard â€” 4 tabs, live KPIs, auto-refresh           â”‚
â”‚  â€¢ Charts: price, volume, spikes, sliding range, anomalies         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ Project Structure

```
data-streaming-app/
â”‚
â”œâ”€â”€ producer.py              # Binance WebSocket â†’ Kafka (Molka)
â”œâ”€â”€ consumer.py              # Verification consumer
â”œâ”€â”€ setup_topics.py          # Kafka topic creation utility
â”œâ”€â”€ docker-compose.yml       # Kafka + Zookeeper + Kafka UI
â”œâ”€â”€ requirements.txt         # Python dependencies (ingestion)
â”‚
â”œâ”€â”€ spark/                   # Spark Structured Streaming (Mehdi)
â”‚   â”œâ”€â”€ app.py               # Streaming entrypoint
â”‚   â”œâ”€â”€ config.py            # Environment-based configuration
â”‚   â”œâ”€â”€ processing.py        # Cleaning + windowed aggregations
â”‚   â”œâ”€â”€ schemas.py           # Kafka message schema
â”‚   â””â”€â”€ requirements.txt     # Spark dependencies
â”‚
â”œâ”€â”€ dashboard.py             # Streamlit dashboard (Nour)
â”œâ”€â”€ ml_model.py              # Isolation Forest + Linear Regression
â”œâ”€â”€ data_loader.py           # Parquet reader with caching
â”œâ”€â”€ requirements_dashboard.txt
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ spark_out/
â”‚       â”œâ”€â”€ tumbling_1m/     # Spark parquet output (1-min windows)
â”‚       â””â”€â”€ sliding_5m_1m/  # Spark parquet output (5-min windows)
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ validate_pipeline.py # End-to-end pipeline validation helper
â”‚
â”œâ”€â”€ README.md                # This file
â””â”€â”€ README_NOUR.md           # Spark output schema for ML/dashboard
```

---

## ðŸ”§ Technologies Used

| Layer | Technology |
|-------|-----------|
| Data Source | Binance WebSocket API (real-time, free, no auth) |
| Message Broker | Apache Kafka + Zookeeper (via Docker) |
| Stream Processing | Apache Spark Structured Streaming 3.5 |
| Machine Learning | scikit-learn (Isolation Forest, Linear Regression) |
| Visualization | Streamlit + Plotly |
| Language | Python 3.12 |
| Infrastructure | Docker + Docker Compose |

---

## ðŸš€ How to Run

### Prerequisites
- Docker & Docker Compose installed
- Python 3.10+
- Java 8+ (required for Spark)

### Step 1 â€” Start Kafka

```bash
docker-compose up -d
```

Wait ~20 seconds. Kafka UI available at: http://localhost:8080

### Step 2 â€” Install dependencies

```bash
pip install -r requirements.txt
pip install -r spark/requirements.txt
pip install -r requirements_dashboard.txt
```

### Step 3 â€” Create Kafka topics

```bash
python setup_topics.py
```

### Step 4 â€” Start the producer (Terminal 1)

```bash
python producer.py
```

### Step 5 â€” Start Spark with parquet sink (Terminal 2)

**Linux / Mac:**
```bash
SINK=parquet python -m spark.app
```

**Windows (PowerShell):**
```powershell
$env:SINK="parquet"
python -m spark.app
```

> âš ï¸ Must be run from the project root (`data-streaming-app/`), not from inside `spark/`

If Spark cannot load the Kafka connector:
```bash
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 spark/app.py
```

### Step 6 â€” Launch the dashboard (Terminal 3)

```bash
streamlit run dashboard.py
```

Open: **http://localhost:8501**

> Wait 1-2 minutes after starting Spark for the first parquet files to appear.

---

## ðŸ“Š Kafka Topics

| Topic | Description |
|-------|-------------|
| `raw_trades` | Raw JSON messages from Binance, no transformation |
| `clean_trades` | Validated, deduplicated, enriched records for Spark |

### Clean trade schema

```json
{
  "symbol":           "BTCUSDT",
  "price":            67432.10,
  "quantity":         0.00120,
  "trade_value_usdt": 80.92,
  "trade_time":       "2026-02-28T16:57:00.123000+00:00",
  "timestamp_ms":     1740754800123,
  "is_buyer_maker":   false,
  "is_price_spike":   false,
  "trade_id":         123456789,
  "source":           "binance"
}
```

---

## ðŸ¤– Machine Learning

### Model 1 â€” Anomaly Detection (Isolation Forest)

Detects unusual market behavior across 1-minute windows.

**Features:** `avg_price`, `trade_count`, `sum_value`, `price_change_pct`, `value_per_trade`, `spike_rate`

**Output:** `anomaly` (bool) + `anomaly_score` (float, lower = more anomalous)

**Contamination rate:** 5% â€” flags the most abnormal windows

### Model 2 â€” Price Prediction (Linear Regression)

Predicts the next 1-minute window's average price.

**Features:** `rolling_mean_5`, `rolling_mean_10`, `rolling_std_5`, `price_change`, `price_change_pct`, `volume_zscore`

**Output:** `predicted_price` â€” next expected average price with direction (ðŸ“ˆ / ðŸ“‰)

Both models are **automatically retrained** on every dashboard refresh as new Spark data arrives.

---

## ðŸ“ˆ Dashboard Features

| Tab | Content |
|-----|---------|
| **Aggregations** | Price over time, trade count, volume, spike count, sliding window range |
| **ML Predictions** | Actual vs predicted price chart, prediction error chart |
| **Anomaly Detection** | Per-symbol anomaly chart, anomaly score over time, anomaly table |
| **Raw Data** | Full tumbling and sliding window dataframes |

Live KPIs: current price, next predicted price, total trades, total volume, ML anomaly status.

Auto-refreshes every N seconds (configurable in sidebar, default 15s).

---

## âœ… Validation

Run the pipeline validation helper to check everything is connected:

```bash
python tools/validate_pipeline.py
```
**28 February 2026** â€” Submitted via GitHub  
Course: Data Streaming Â· M2 BDIA Â· UniversitÃ© Paris Dauphine  
Professor: Nour ElHouda Ben Ali
